{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Projeto: Chat com PDF usando RAG e IA Generativa\n",
        "\n",
        "**Autor:** Thauana Farias\n",
        "**Tech Stack:** Python, LangChain, Groq API (Llama 3), FAISS, HuggingFace.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Objetivo\n",
        "Este notebook implementa um pipeline de **RAG (Retrieval-Augmented Generation)**. O objetivo √© permitir que uma IA \"leia\" um documento PDF privado e responda perguntas baseadas exclusivamente no conte√∫do desse arquivo, evitando alucina√ß√µes.\n",
        "\n",
        "### ‚öôÔ∏è Como funciona?\n",
        "1. **Ingest√£o:** O PDF √© carregado e transformado em texto.\n",
        "2. **Chunking:** O texto √© dividido em peda√ßos menores (chunks).\n",
        "3. **Embeddings:** Os peda√ßos s√£o convertidos em vetores num√©ricos.\n",
        "4. **Vector Store:** Os vetores s√£o armazenados em um banco de dados local (FAISS).\n",
        "5. **Retriever + LLM:** Quando o usu√°rio faz uma pergunta, o sistema busca os trechos relevantes e envia para o Llama 3 gerar a resposta."
      ],
      "metadata": {
        "id": "NwjZZWMv27ZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configura√ß√£o do Ambiente e Bibliotecas\n",
        "Instala√ß√£o das depend√™ncias necess√°rias. Estamos utilizando vers√µes espec√≠ficas do `LangChain` para garantir estabilidade e compatibilidade com o Google Colab."
      ],
      "metadata": {
        "id": "cCP1178y3X-o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNJ9uXULt6Na"
      },
      "outputs": [],
      "source": [
        "#!pip install -q langchain==0.1.20 langchain-community==0.0.38 langchain-core==0.1.52 langchain-groq faiss-cpu pypdf sentence-transformers \"numpy<2\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configura√ß√£o de Seguran√ßa (API Key)\n",
        "Aqui configuramos a chave da API da **Groq Cloud**, que nos dar√° acesso ao modelo **Llama 3**.\n",
        "> **Nota:** Utilizei `getpass` para inserir a chave de forma segura, sem deix√°-la exposta no c√≥digo."
      ],
      "metadata": {
        "id": "b1Kf45Gb3dXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Imports\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# 1. Configura√ß√£o da Chave (Segura)\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Insira sua chave da Groq: \")\n",
        "\n",
        "# 2. Valida√ß√£o Inicial (O Pulo do Gato üêà)\n",
        "# Se o arquivo n√£o existir OU a chave estiver errada, a gente PARA aqui.\n",
        "arquivo_pdf = \"/content/documento.pdf\"\n",
        "\n",
        "if not os.path.exists(arquivo_pdf):\n",
        "    raise FileNotFoundError(f\"‚ùå PARE! O arquivo '{arquivo_pdf}' n√£o foi encontrado. Suba o PDF antes de continuar.\")\n",
        "\n",
        "if \"gsk_\" not in os.environ[\"GROQ_API_KEY\"]:\n",
        "    raise ValueError(\"‚ùå PARE! A chave da API parece inv√°lida (tem que come√ßar com 'gsk_').\")\n",
        "\n",
        "print(\"‚úÖ Configura√ß√£o OK! O ambiente est√° pronto para processar.\")"
      ],
      "metadata": {
        "id": "POCA4YQB38yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Ingest√£o e Processamento de Dados (ETL)\n",
        "Nesta etapa, realizamos duas opera√ß√µes cr√≠ticas:\n",
        "1. **Carregamento:** O `PyPDFLoader` extrai o texto bruto do arquivo.\n",
        "2. **Text Splitting (Fatiamento):** Dividimos o texto em blocos de 1000 caracteres. Isso √© necess√°rio porque os modelos de IA (LLMs) possuem um limite de entrada (janela de contexto). Usamos um `overlap` (sobreposi√ß√£o) para n√£o perder o sentido entre os cortes."
      ],
      "metadata": {
        "id": "SxDjmmu54Izo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Como j√° validamos na c√©lula anterior, podemos rodar direto:\n",
        "\n",
        "# 1. Configurar IA\n",
        "print(\"Iniciando C√©rebro...\")\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
        "\n",
        "# 2. Carregar PDF\n",
        "print(\"Lendo PDF...\")\n",
        "loader = PyPDFLoader(arquivo_pdf)\n",
        "docs = loader.load()\n",
        "\n",
        "# 3. Dividir (Chunking)\n",
        "print(\"Fatiando texto...\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "# 4. Vetorizar (Embeddings)\n",
        "print(\"Criando mem√≥ria vetorial...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "# 5. Criar a Chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever()\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Processamento conclu√≠do! O modelo aprendeu o conte√∫do do PDF.\")"
      ],
      "metadata": {
        "id": "WzExFpIxSLvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Interface de Chat Interativa\n",
        "Para finalizar, criamos um **loop de intera√ß√£o** (`while True`) que permite conversar continuamente com o documento, sem precisar rodar o c√≥digo repetidas vezes.\n",
        "\n",
        "Implementamos tamb√©m:\n",
        "* **Condi√ß√£o de Sa√≠da:** O usu√°rio pode digitar \"sair\" ou \"fim\" para encerrar o script graciosamente.\n",
        "* **Tratamento de Erros:** Utilizamos blocos `try-except` para garantir que, caso a API falhe ou ocorra um erro de rede, o programa n√£o trave (crash), mas sim avise o usu√°rio e continue rodando."
      ],
      "metadata": {
        "id": "woFzRy9J5WaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap"
        "from IPython.display import display, Markdown"
        "print(\"ü§ñ CHAT INICIADO (Digite 'sair' para encerrar)\")\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "while True:\n",
        "\n",
        "    pergunta_usuario = input(\"\\nüë§ Voc√™: \")\n",
        "\n",
        "    if pergunta_usuario.lower() in [\"sair\", \"exit\", \"fim\"]:\n",
        "        print(\"üëã Encerrando chat...\")\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        print(\"ü§ñ Pensando...\")\n",
        "        resposta = qa_chain.invoke(pergunta_usuario)\n",
        "        print(f\"ü§ñ IA: {resposta['result']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro: {e}\")"
      ],
      "metadata": {
        "id": "9wThMpU2yUSG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
